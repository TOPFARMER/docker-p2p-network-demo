PBFT步骤：
1. 实现服务器能传递消息
2. 服务器查询publicKey后应有一个缓存表: supervisors{}



### 逻辑：
#### 连接层：
1. 连接时区分server与client，并登记各个server的publicKey
   
   补充：验证环节之后需要加强
   
2. peers列表收到之后进行排序，用于确定每次发起请求的view id
3. 为什么使用websocket连接？
   全双工，可以设置连接门槛，可以确认消息来源，省去签名计算量。


#### 发信环节
1. 为什么request要向全网广播
   
   首先，我的技术无法实现在没有连接上系统的情况下确定当前系统内部viewId。

   其次，全网广播request可以让非主节点先行计算区块，加快系统运行速度。

2. request校验不需要校验来源，只需要检查审查员的签名是否合法即可。

   拜占庭客户端行为：没有向主节点发request，反而只向部分的服务器发送了request，该如何处理？

3. orderedRequest的“减肥”
   
   首先，第一步是根据时间戳直接与本地相同时间戳的区块缓存的区块对照（直接对照哈希即可）。如果本地区块缓存没有这个区块，那么校验信息中附带的request的合法性，合法则生成区块，对照哈希。如此说来，OR的信息只需要主节点自身生成的区块哈希以及request的信息即可。

```
  // 每次先收request能预处理，加快处理时间
  // 需要验证生成的区块是否与本地生成的区块一致
  // 如果本地缓存中没有找到该时间戳的request，说明该block的request还没有收到
  // 此时有两种情况: 1.客户端的request还没送到 2.该block是恶意攻击
  // 判断的情况是：查找requestCache表，如果有则对照两个block
  // 没有则利用orderedRequest发来的request信息（先校验有效性）
  // 然后生成区块，再与发来的block对照
  // 若此后再次收到request，直接生成区块压入即可
  // 因为这个request若为审查员已签的，则生成区块一定与此前通过校验的区块一致
```

4. 主节点无响应
   
   为了保持系统活性，在原Zyzzyva算法中，若主节点发生人为延迟攻击，客户端必须负责再次往所有服务器发送request消息，然后客户端再往主节点发送confirmRequest消息，试图获得主节点的回复。
   
   但在本实现中，由于每次客户端都向全体服务器广播request请求，所以在每个服务器收到客户端的request后，设置了500ms的超时范围，用于容纳网络延迟的时间。
   
   若在该500ms时间范围内仍没有收到主节点的orderedRequest消息，则认为主节点丢失了客户端的request消息，并把本地接收的request消息发送给主节点，等待回复，若再次等待500ms后主节点没有回应，则认为主节点网络环境差，或发生了拜占庭错误，从而向全网广播IHATEPRIMARY消息。


5. 内部发起的视图转换
   
   在Zyzzyva实现中，视图变更是由客户端（收集器）发起的，原因是在发起request时，客户端只向主节点发信，其他副本无法得知request的准确内容。

   本实现中，request向所有节点广播，所以整个系统的视图情况是不向外暴露的。原始的PBFT算法也是如此，视图变更应该由内部节点发起，客户端无需获知内部情况。

   **关于检查点（checkpoint）?**
   
   检查点(checkpoint)在各种类型的PBFT算法中有不同的称呼（水位、提交证书等），其用途是确认在拜占庭错误发生时，系统中大于2/3的节点所同意的正确区块位置，该位置一旦确立，则说明整个系统在该位置之前的所有区块都是一致的。
   
   原Zyzzyva的实现中，checkpoint在客户端收取正确回应条数```CP(currect reponse)```， ```2f + 1 < CP < 3f + 1```时产生，产生过程为服务器收取客户端发起的commit信息，确认结果后给客户端发回localCommit。
   
   但是原实现在本项目中有其局限性。
   
   首先，若一直没有发生拜占庭错误：
   
   1. 突然，客户端收取有效回复条数 ```CP < 2f + 1```， 由于系统中错误节点必不多于```f```，可认为主节点发生拜占庭错误。发起视图变更，最后确定历史记录时，由于系统中不存在检查点，可以认为迄今为止都一致则无需对照缓存。

   带来的问题是：系统一直都无法清理历史记录缓存，无法进行垃圾回收。

   1. 在一段长时间之前，系统发生过非主节点引起的拜占庭错误。检查点在当前区块前很远的地方，此时一旦发生主节点的拜占庭错误，则需要启动视图变更。在对照历史时，历史记录缓存过于庞大，系统试视图变更过程极其缓慢，效率底下。
   
   **Commit消息的发送时机**

   明确一点，checkpoint是服务于视图改变的。于是该机制在目前项目中施行可以针对性改造。

   接上文，若系统发生拜占庭错误,则客户端向全网广播commit，为了减少客户端（收集器）的计算量，以及commit的数据量，服务器收到commit后各自发送localCommit（包含区块高度，区块哈希以及本机签名）并收集，确认检查点，清理历史记录缓存，生成检查点提交证书（localCommit的签名集合），检查签名的真实性，不回复。

   为避免发生上述情况2,必须提出其他历史记录缓存清理机制：服务器用区块高度定位，检查当前区块高度，如果高度距离上一检查点达到某值，则自动发起localCommit广播，让其他服务器收取证书，同时统计localCommit，并进行历史记录缓存清理，生成检查点提交证书，检查签名的真实性。

   原因解析：如果非拜占庭客户端发出commit，说明客户端本身收到了至少2/3的正确区块，此时不需要服务器往客户端回发localCommit。如果拜占庭客户端发出commit，服务器需要知道当前系统是否有2/3或以上的节点确认该检查点正确，他们需要给各自发送localCommit统计并确认。

   **视图变更时的安全处理**

   一旦发生视图变更，代价会很大。首先新主节点需收集所有的副本节点的历史记录。目前的localCommit没有实现签名统计，只实现了根据节点IP地址统计。

   一旦发生视图变更，新主节点根据所有的历史记录，选出所有满足有大于```f + 1```个相同副本的历史，并广播给所有节点更新他们本地的历史缓存。
  

6. 其他补充
   
   **服务器分配密钥对的必要性**    
  
   密钥对可以辨别发信主机，在出错时能定位拜占庭错误点，恢复主机。同时私钥签名能确保多条消息非同节点所发。

   **外部查阅**

   由于整个系统中正确历史必须大于以大于```2f + 1```个相同的副本证明，在系统内部目前使用各自的Websocket的IP来确认不同的节点承认该历史。
   
   在系统外部，同时考虑到客户端有可能表现出拜占庭错误的行为下，需要查询到的历史由各个节点签名，才能确定正确的历史。

   **局限性**

   拜占庭客户端：Zyzzyva算法虽然提高了PBFT的性能，但同时也增强了客户端的权力，使得客户端成为了整体算法一部分。

   及时恢复：系统只能容忍最多f个拜占庭节点。
   
   首先考虑单个节点在某次区块生成中因为网络原因，没有收到request以及orderRequest，那么之后生成的区块由于高度与历史区块不连续，它都将认为是非法的，整个节点会对后续所有的区块不响应，节点将形成拜占庭节点，此时我们需要及时将节点退出网络。重置后再加入网络。
   
   若不及时恢复，网络中恰有f个节点呈现拜占庭错误行为，此时系统会崩溃，再不能保障内部数据安全可靠。

   公钥安全：由于需要将个人身份与公钥绑定，需要公钥的保存数据库绝对安全，才能保证个人与公钥的准确对应。若数据库被入侵篡改，个人对应的密钥对被更换，或添加了其他的管理用户，则会出现冒签，伪造数据等现象。
   